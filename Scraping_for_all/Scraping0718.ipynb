{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import dateutil.parser as parser\n",
    "import time\n",
    "\n",
    "class News:\n",
    "    Article = ''\n",
    "    Title = ''\n",
    "    Author = ''\n",
    "    OriginalContent = ''\n",
    "    CreatedDate = ''\n",
    "    FetchedDate = ''\n",
    "    ArticleUrl = ''\n",
    "    LastUsed = ''\n",
    "    \n",
    "    def __init__(self, article , title , author , originalcontent , createddate , fetcheddate , articleurl , lastused ):\n",
    "        self.Article = article\n",
    "        self.Title = title\n",
    "        self.OriginalContent = originalcontent\n",
    "        self.CreatedDate = createddate\n",
    "        self.FetchedDate = fetcheddate\n",
    "        self.ArticleUrl = articleurl\n",
    "        self.LastUsed = lastused\n",
    "        self.Author = author\n",
    "        \n",
    "def scraping(href):\n",
    "    headers = {'user-agent' : 'Mozilla/5.0'}\n",
    "    if (requests.get(href, headers = headers)):\n",
    "        source = requests.get(href, headers = headers)\n",
    "        if (BeautifulSoup(source.content, \"lxml\")):\n",
    "            soup = BeautifulSoup(source.content, \"lxml\").body \n",
    "            news = News(None, None, None, None, None, None, None, None)\n",
    "            date = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n",
    "            news.fetchedDate = date\n",
    "            news.lastUsed = date \n",
    "            news.articleUrl = href\n",
    "            \n",
    "            class_list_main = [    \n",
    "                        'col-md-12',\n",
    "                        'content-wrapper clearfix detail-block-md',\n",
    "                        'entry-content',\n",
    "                        'column column-two-third site-content',\n",
    "                        'large-8 medium-12 columns first',\n",
    "                        'article',\n",
    "                        'article_body'\n",
    "                        'container',\n",
    "                        'span8',\n",
    "                        'primary',\n",
    "                        'l-container',\n",
    "                        'row'\n",
    "            ]\n",
    "            \n",
    "            for i in range(len(class_list_main)):\n",
    "                if (soup.findAll(\"\",{'class':class_list_main[i]})):\n",
    "                    print(class_list_main[i])\n",
    "                    content_list = soup.findAll(\"\",{'class':class_list_main[i]})\n",
    "#                     print (content_list)\n",
    "                    for content in content_list:\n",
    "                        scrape_details(content,news)\n",
    "                        break\n",
    "                else:\n",
    "                    content = soup\n",
    "                    scrape_details(content,news)\n",
    "    \n",
    "            print ('URL********************************************************')\n",
    "            print (news.articleUrl)\n",
    "            print ('TITLE********************************************************')\n",
    "            print (news.title)\n",
    "            print ('DATE********************************************************')\n",
    "            print (news.createdDate)\n",
    "            print ('AUTHOR********************************************************')\n",
    "            print (news.author)\n",
    "            print ('ARTICLE********************************************************')\n",
    "            print (news.article) \n",
    "        else:\n",
    "            print ('can not get the information!')\n",
    "    else:\n",
    "        print ('can not connect to the web page!')\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_details(content,news):  \n",
    "    news.content = content\n",
    "    print (news.content)\n",
    "    print ('title')\n",
    "    class_list_title = [\n",
    "                'entry-title',\n",
    "                'entry-title single-title',\n",
    "                'col-lg-9 col-md-8',\n",
    "                'name post-title entry-title',\n",
    "                'articleTitle',\n",
    "                'artTitle',\n",
    "                'blog-title',\n",
    "                'postarea',\n",
    "                'full-page-article left-col',\n",
    "                'post_title',\n",
    "                'post-title',\n",
    "                'pg-headline',\n",
    "                'content-header ',\n",
    "                'field-subhead'\n",
    "            ] \n",
    "    print ('title')\n",
    "    for i in range(len(class_list_title)):\n",
    "        if (content.find('',{'class':class_list_title[i]})):\n",
    "            print (class_list_title[i])\n",
    "            news.title = content.find('',{'class':class_list_title[i]}).text\n",
    "            print (news.title)           \n",
    "            break\n",
    "        else:\n",
    "            news.title = None\n",
    "\n",
    "    class_list_paragraph = [\n",
    "                'ArticleBody__articleBody___1GSGP',\n",
    "                'art-postcontent',\n",
    "                'articles2',\n",
    "                'article-inner',\n",
    "                'article-text',                    \n",
    "                'article-content',\n",
    "                'article-copy',\n",
    "                'article_body',\n",
    "                'articleContentData',\n",
    "                'b-item__description',\n",
    "                'content-main',\n",
    "                'content-text', \n",
    "                'entry-content clearfix',\n",
    "                'entry-content full-content',\n",
    "                'entry clearfix',        \n",
    "                'entry-content', \n",
    "                'entry_content',\n",
    "                'entry-content print-only',\n",
    "                'entry-body',\n",
    "                'entry',                                       \n",
    "                'entry entry-content',\n",
    "                'event-text', \n",
    "                'entry-content-text',\n",
    "                'field-item even',\n",
    "                'field-body',\n",
    "                'infopage-news',\n",
    "                'td-post-content', \n",
    "                'the-content cf',\n",
    "                'thecontent',\n",
    "                'td-post-content td-pb-padding-side',\n",
    "                'ntText',                          \n",
    "                'post-single',                     \n",
    "                'post-content entry-content cf',   \n",
    "                'post-body entry-content',        \n",
    "                'post_entry',                      \n",
    "                'post-content clearfix', \n",
    "                'post-bodycopy clearfix',\n",
    "                'post-single-content box mark-links',\n",
    "                'post-9141 post type-post status-publish format-standard has-post-thumbnail hentry category-world tag-fake-news tag-featured tag-satire',\n",
    "                'vw-post-content clearfix',\n",
    "                'wpb_wrapper',\n",
    "                'single-box clearfix entry-content',\n",
    "                'sqs-block-content',\n",
    "                'left relative',\n",
    "                'metadata__byline__author'\n",
    "    ]\n",
    "\n",
    "    news.article = []\n",
    "    for i in range(len(class_list_paragraph)):\n",
    "        if (content.find('div',{'class': class_list_paragraph[i]})): \n",
    "#             print (class_list_paragraph[i])\n",
    "            paragraphs = content.find('',{'class': class_list_paragraph[i]})\n",
    "            if (paragraphs.findAll('p')):\n",
    "#                     print (paragraphs)\n",
    "                for paragraph in paragraphs.findAll('p'):\n",
    "                    news.article.append(paragraph.text)\n",
    "                    print (news.article)\n",
    "                break\n",
    "        else:    \n",
    "            news.article = []\n",
    "\n",
    "    class_list_author = [\n",
    "                'postmetadata',\n",
    "                'author-article-link',\n",
    "                'field-author',\n",
    "                'name',\n",
    "                'url fn n',\n",
    "                'author vcard',\n",
    "                'author left-edge',\n",
    "                'author has-bio',\n",
    "                'author',\n",
    "                'artAuthor',\n",
    "                'post-author',\n",
    "                'meta pf-author',\n",
    "                'author-name vcard fn'\n",
    "    ]\n",
    "\n",
    "    for i in range(len(class_list_author)):\n",
    "        if (content.find('',{'class': class_list_author[i]})):\n",
    "#             print (class_list_author[i])\n",
    "            news.author = content.find(\"\",{'class':class_list_author[i]}).text \n",
    "            print (news.author)\n",
    "            break\n",
    "        else:\n",
    "            news.author = None\n",
    "\n",
    "\n",
    "    class_list_date = [\n",
    "                'entry-date published',\n",
    "                'entry-date published updated',\n",
    "                'content-published-mobile',\n",
    "                'entry-date',\n",
    "                'entry-meta',\n",
    "                'entry-meta-date updated',\n",
    "                'byline byline-left ',\n",
    "                'pub_date',\n",
    "                'post-date updated',\n",
    "                'timestamp',\n",
    "                'time',\n",
    "                'article-details',\n",
    "                'article_date',\n",
    "                'artData',\n",
    "                'field-post-date',\n",
    "                'update-time'\n",
    "    ]\n",
    "\n",
    "    for i in range(len(class_list_date)):\n",
    "        if (content.find('',{'class': class_list_date[i]})):\n",
    "            print(class_list_date[i])\n",
    "            news.createdDate = content.find('',{'class': class_list_date[i]})\n",
    "            print (news.createdDate.text)\n",
    "            try:\n",
    "                news.createdDate = (parser.parse(news.createdDate['datetime'])).isoformat()\n",
    "            except:\n",
    "                news.createdDate = (parser.parse(news.createdDate.text)).isoformat()\n",
    "            break\n",
    "        else:\n",
    "            news.createdDate = None\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "link = 'http://time.com/4867932/donald-trump-investigate-robert-mueller-aides/'\n",
    "# link = 'http://www.thedailysheeple.com/astronomers-detect-strange-radio-signals-from-nearby-star_072017'\n",
    "# FAKE:\n",
    "# link = 'http://awm.com/will-this-latest-blunder-be-the-final-nail-in-the-coffin-for-the-worst-show-in-tv-history/?utm_medium=homepage&utm_source=homepage1'\n",
    "# link = 'http://americannews.com/beware-new-doll-meant-indoctrinate-sharia-law-children-coming-home/'\n",
    "# UNRELIABLE:\n",
    "# link = 'http://www.anonews.co/cop-drugs-frame/'\n",
    "# link = 'http://anonhq.com/london-terror-attacks-brits-celebrating-beer-hero/'\n",
    "# SATIRE\n",
    "# link = 'http://empirenews.net/hillary-clinton-undergoes-sex-change-operation-so-she-has-a-better-chance-at-winning-2020-election/'\n",
    "# link = 'http://realnewsrightnow.com/2017/05/texas-lawmaker-called-ice-mother-law-dinner-table-dispute/'\n",
    "# Conspiracy:\n",
    "# link = 'http://anotherdayintheempire.com/distraction-du-jour-trump-punches-cnn/'\n",
    "# link = 'http://conservativefiringline.com/va-removes-top-2-officials-manchester-va-hospital/'\n",
    "# HATE:\n",
    "# link = 'http://www.frontpagemag.com/fpm/267321/nevertrump-nostalgia-hillary-never-was-daniel-greenfield'\n",
    "# link = 'http://www.vdare.com/articles/said-in-spanish-dual-citizen-actress-activist-tells-illegals-how-not-to-get-arrested-a-new-app-for-dreamers-the-america-thing-etc'\n",
    "# BIAS\n",
    "# link = 'http://www.americanthinker.com/articles/2017/07/europe_downward_now_the_bavarians_want_to_leave.html'\n",
    "# link = 'http://100percentfedup.com/watch-rep-steve-kings-bombshell-answer-trumps-wall-hasnt-built-yet-video/'\n",
    "# Junk:\n",
    "# link = 'http://www.celebtricity.com/nicki-minaj-files-10m-lawsuit-against-remy-ma-for-using-her-voice-in-diss-song-on-itunes/'\n",
    "# link = 'https://www.scoopwhoop.com/india-inhuman-rape-cases/#.6j9dfreps'\n",
    "# Political:\n",
    "# link = 'http://thelastlineofdefense.org/breaking-president-trump-wants-you-to-know-the-truth-about-agenda-21/'\n",
    "# link = 'http://www.defenddemocracy.press/one-week-after-mosuls-liberation-horror-of-us-siege-continues-to-unfold/'\n",
    "# CLICKBAIT\n",
    "# link = 'http://americablog.com/2017/07/cbo-gop-obamacare-repeal-22m-uninsured-13k-deductible-premiums-soar-older.html'\n",
    "# link = 'http://americanlookout.com/rms-corruption-more-obama-officials-scrutinized-in-unmasking-probe-video/'\n",
    "scraping(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3k",
   "language": "python",
   "name": "py3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
